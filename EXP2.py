# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9AgddVpChV96Z-5oURXjTRX1mM0h95E
"""

import numpy as np
import matplotlib.pyplot as plt

class SingleNeuron:
    def __init__(self, input_size):
        # Initialize weights and bias randomly
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()

    def step_function(self, x):
        return 1 if x >= 0 else 0

    def forward(self, x):
        linear_output = np.dot(x, self.weights) + self.bias
        return self.step_function(linear_output)


# AND gate truth table
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,0,0,1])

# Train manually for AND (weights and bias can be fixed for AND gate)
neuron = SingleNeuron(2)
neuron.weights = np.array([1,1])   # Both inputs equally important
neuron.bias = -1.5                 # Ensures output is 1 only if both inputs = 1

print("Single Neuron (AND Gate) Results:")
for i in range(len(X)):
    print(f"Input: {X[i]}, Output: {neuron.forward(X[i])}, Expected: {y[i]}")

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def forward_pass(x, w1, b1, w2, b2):
    # Hidden layer
    z1 = np.dot(x, w1) + b1
    a1 = sigmoid(z1)

    # Output layer
    z2 = np.dot(a1, w2) + b2
    a2 = sigmoid(z2)
    return a1, a2

# Pre-trained weights (manually chosen to solve XOR)
w1 = np.array([[20,20],[20,20]])   # 2x2 hidden layer weights
b1 = np.array([-10,30])           # Bias for hidden layer
w2 = np.array([[20],[-20]])       # Output layer weights
b2 = np.array([-10])              # Bias for output layer

print("\nFeedforward XOR Results:")
for i in range(len(X)):
    _, output = forward_pass(X[i], w1, b1, w2, b2)
    print(f"Input: {X[i]}, Output: {np.round(output[0])}, Expected: {X[i][0] ^ X[i][1]}")

class MLP:
    def __init__(self, input_size, hidden_size, output_size, lr=0.1):
        # Xavier Initialization
        self.w1 = np.random.randn(input_size, hidden_size) * np.sqrt(2./input_size)
        self.b1 = np.zeros((hidden_size,))
        self.w2 = np.random.randn(hidden_size, output_size) * np.sqrt(2./hidden_size)
        self.b2 = np.zeros((output_size,))
        self.lr = lr

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_deriv(self, x):
        return x * (1 - x)

    def forward(self, x):
        self.z1 = np.dot(x, self.w1) + self.b1
        self.a1 = self.sigmoid(self.z1)

        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2

    def backward(self, x, y):
        error = y - self.a2
        d_a2 = error * self.sigmoid_deriv(self.a2)

        error_hidden = np.dot(d_a2, self.w2.T)
        d_a1 = error_hidden * self.sigmoid_deriv(self.a1)

        # Update weights
        self.w2 += self.lr * np.dot(self.a1.T, d_a2)
        self.b2 += self.lr * np.sum(d_a2, axis=0)

        self.w1 += self.lr * np.dot(x.T, d_a1)
        self.b1 += self.lr * np.sum(d_a1, axis=0)

        return np.mean(np.square(error))  # Mean Squared Error

# XOR dataset
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])

mlp = MLP(input_size=2, hidden_size=4, output_size=1, lr=0.1)

epochs = 10000
losses = []
for epoch in range(epochs):
    output = mlp.forward(X)
    loss = mlp.backward(X, y)
    losses.append(loss)

# Plot training loss
plt.plot(losses)
plt.title("Training Loss (MSE)")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# Evaluate accuracy
predictions = np.round(mlp.forward(X))
accuracy = np.mean(predictions == y)

print("\nMLP with Backpropagation Results (XOR):")
for i in range(len(X)):
    print(f"Input: {X[i]}, Predicted: {predictions[i][0]}, Expected: {y[i][0]}")

print(f"\nFinal Accuracy: {accuracy*100:.2f}%")